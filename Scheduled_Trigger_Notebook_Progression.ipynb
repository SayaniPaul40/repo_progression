{"cells":[{"cell_type":"markdown","source":["**'Scheduled_Trigger_Notebook_Progression' Notebook :**\n","\n","The scheduler notebook runs daily as part of the pipeline **'pl_silver_to_gold'**. This notebook is a scheduler notebook that triggers biweekly unseen prediction based on the dates mentioned in config. The notebook can be scheduled as per user's convenience, but the functions will be executed only on the mentioned dates, and on the other days, running the notebook won't trigger any function for execution.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"db658b50-474a-4214-990d-db94ec9659eb"},{"cell_type":"markdown","source":["**1. Importing necessary packages and defining log function :**\n","\n",">This codeblock imports the necessary packages to run this notebook, also defines the function that's used to generate log."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"027b16db-2cd3-414a-8f2c-a5ba62252fb9"},{"cell_type":"code","source":["import logging\n","from enum import Enum\n","import pandas as pd\n","from notebookutils import mssparkutils\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.sql.utils import AnalysisException\n","from pyspark.sql import Row\n","import json\n","from datetime import datetime, timedelta\n","\n","# Set up logging configuration\n","\n","class LogLevel(Enum):\n","    INFO = \"INFO\"\n","    ERROR = \"ERROR\"\n","    WARNING = \"WARNING\"\n","\n","log_array = []\n","\n","# Notebook name for log entries\n","notebook_name = \"Scheduled_Trigger_Notebook_Progression\"\n","\n","# Logging function with status options: In Progress, Successful, Failed\n","def logging(type_, message, status):\n","    msg = {\n","        \"timestamp\": str(pd.Timestamp.now()),  # Timestamp column\n","        \"log_type\": type_,                     # Log type column\n","        \"status\": status,                      # Status column\n","        \"description\": message,                # Description column\n","        \"notebook\": notebook_name              # Notebook name column\n","    }\n","    log_array.append(msg)\n","    print(msg)\n","\n","\n","# Function for storing log table\n","def createLogfile():\n","    try:\n","        # Initialize Spark session\n","        spark = SparkSession.builder.appName(\"ScheduleTriggerNotebook\").getOrCreate()\n","\n","        # Convert the log_array to a Spark DataFrame\n","        log_rows = [Row(**item) for item in log_array]  # Unpack the dictionary for Row entries\n","        new_log_df = spark.createDataFrame(log_rows)\n","\n","        # Define the existing log table name\n","        with open(\"/lakehouse/default/Files/progression_config_template/progression_config.json\") as config_file:\n","            config = json.load(config_file)\n","        \n","        # Define the existing log table name\n","        existing_log_table_name =config[\"progression_log\"]\n","    \n","        # Write the new logs to the table (overwrite)\n","        new_log_df.write.mode(\"append\").saveAsTable(existing_log_table_name)\n","\n","        logging(LogLevel.INFO.value,f\"Logging information saved in table '{existing_log_table_name}'\",status=\"Successful\")\n","\n","        print(f\"Logging information saved in table '{existing_log_table_name}'.\")\n","        \n","    except Exception as e:\n","        logging(LogLevel.ERROR.value,f\"Error appending log table: {e}\",status=\"Failed\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"98cabff6-499e-4ae9-919c-8fd87bcf6101","normalized_state":"finished","queued_time":"2024-12-03T14:35:10.3431826Z","session_start_time":null,"execution_start_time":"2024-12-03T14:35:15.7759242Z","execution_finish_time":"2024-12-03T14:35:18.3885125Z","parent_msg_id":"47df19b0-074d-499f-9632-23d9d24719d8"},"text/plain":"StatementMeta(, 98cabff6-499e-4ae9-919c-8fd87bcf6101, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6460856-ac1f-4d8c-a719-a1617d57b0bf"},{"cell_type":"markdown","source":["**2. Fetching Config File :**\n","\n","> This codeblock fetches the config file. The config file is necessary as it contains scheduled dates for Bi-weekly unseen prediction."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"086f8f63-5581-4057-ad9f-bf3b0f171548"},{"cell_type":"code","source":["# Load the Config JSON file\n","try:\n","    logging(LogLevel.INFO.value,f\"Running Scheduled Trigger Notebook for Progression.\",status=\"Successful\")\n","    with open(\"/lakehouse/default/Files/progression_config_template/progression_config.json\") as time_config:\n","                config = json.load(time_config)\n","except Exception as e:\n","    logging(LogLevel.ERROR.value,f\"Error loading config file: {e}\",status=\"Failed\")\n","    createLogfile()\n","    raise e                "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"98cabff6-499e-4ae9-919c-8fd87bcf6101","normalized_state":"finished","queued_time":"2024-12-03T14:35:10.5906951Z","session_start_time":null,"execution_start_time":"2024-12-03T14:35:18.935061Z","execution_finish_time":"2024-12-03T14:35:19.189647Z","parent_msg_id":"63c7d58d-4ddd-4b7b-813b-d80a85970e0b"},"text/plain":"StatementMeta(, 98cabff6-499e-4ae9-919c-8fd87bcf6101, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'timestamp': '2024-12-03 14:35:18.809625', 'log_type': 'INFO', 'status': 'Successful', 'description': 'Running Scheduled Trigger Notebook for Progression.', 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4c0eec70-4b9e-4b74-a9eb-de184ca5a021"},{"cell_type":"markdown","source":["**3. Bi-weekly Unseen Batch Prediction Trigger :**\n","\n",">From config, the codeblock fetches the date for \"bl_last_biweekly_run\" and \"dl_last_biweekly_run\". Then it gets today's date. If today's date and last_biweekly_run date has a difference of less than 14 days, the function doesn't get executed.\n","\n",">In case today's date and last_biweekly_run date has a difference of more than 14 days, the codeblock runs corresponding unseen prediction notebook and new predictions are generated for the unseen student records. In the end \"bl_last_biweekly_run\" and \"dl_last_biweekly_run\" gets updated with today's date, and same cycle runs again 14 days later."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"49116074-da35-404b-ab01-2ca59b7adf12"},{"cell_type":"code","source":["# Extract the last biweekly run date\n","bl_last_biweekly_run = datetime.strptime(config[\"pipeline_schedule\"][\"bl_last_biweekly_run\"], '%Y-%m-%d')\n","\n","# Get today's date\n","today = datetime.today()\n","\n","# Check if two weeks (14 days) have passed since the last biweekly run\n","if today.date() >= (bl_last_biweekly_run.date() + timedelta(days=14)):\n","    logging(LogLevel.INFO.value,f\"Day for new unseen batch prediction : {today}. Starting Batch Prediction notebook...\",status=\"Successful\")\n","    ####################################\n","    # Execute Notebook: Unseen Prediction\n","    ####################################\n","    execute_notebook_name = \"FT_Progression_Analysis_Unseen\"\n","    try:\n","        logging(LogLevel.INFO.value,f\"Starting '{execute_notebook_name}' notebook...\",status=\"Successful\")\n","\n","        # Run the data_extraction_validation notebook\n","        result = mssparkutils.notebook.run(execute_notebook_name, 86000)\n","        \n","        logging(LogLevel.INFO.value,f\"'{execute_notebook_name}' notebook completed successfully.\",status=\"Successful\")\n","        logging(LogLevel.INFO.value,f\"New predictions generated for new students.\",status=\"Successful\")\n","\n","        # Update the last_biweekly_run date to today and save it back to the JSON file\n","        config[\"pipeline_schedule\"][\"bl_last_biweekly_run\"] = today.strftime('%Y-%m-%d')\n","\n","        with open(\"/lakehouse/default/Files/progression_config_template/progression_config.json\", \"w\") as bi_weekly_config:\n","            json.dump(config, bi_weekly_config)\n","            bi_weekly_config.close()\n","\n","        logging(LogLevel.INFO.value,f\"Bi-weekly run date updated for BL.\",status=\"Successful\")\n","\n","    except Exception as e:\n","        logging(LogLevel.ERROR.value,f\"Error running '{execute_notebook_name}' notebook: {e}\",status=\"Failed\")\n","        createLogfile()\n","        raise e"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"98cabff6-499e-4ae9-919c-8fd87bcf6101","normalized_state":"finished","queued_time":"2024-12-03T14:35:10.7549235Z","session_start_time":null,"execution_start_time":"2024-12-03T14:35:19.6106086Z","execution_finish_time":"2024-12-03T14:36:36.0457462Z","parent_msg_id":"3b37f05c-1c7d-4c2a-bd79-a20afeb9bc8f"},"text/plain":"StatementMeta(, 98cabff6-499e-4ae9-919c-8fd87bcf6101, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'timestamp': '2024-12-03 14:35:19.559425', 'log_type': 'INFO', 'status': 'Successful', 'description': 'Day for new unseen batch prediction : 2024-12-03 14:35:19.559155. Starting Batch Prediction notebook...', 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\n{'timestamp': '2024-12-03 14:35:19.559510', 'log_type': 'INFO', 'status': 'Successful', 'description': \"Starting 'FT_Progression_Analysis_Unseen' notebook...\", 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.mssparkutilsrun-result+json":{"run_id":"6452f1ef-894f-4b74-960d-b647dba786ca","in_pipeline":false,"notebook_name":"FT_Progression_Analysis_Unseen","snapshot_path":"","error":null,"session_id":"98cabff6-499e-4ae9-919c-8fd87bcf6101","spark_pool":"Starter Pool","capacity_id":"ADAB8840-6773-4F35-A058-47ECC22850EA","workspace_id":"21762bb0-73d6-4005-852d-a2d1ab125ffe","root_artifact_id":"b084b002-b542-4af4-b979-cbd3ec86c347","artifact_id":"a310903f-e011-4b07-8698-822594ffd098","snapshot_status":"success","snapshot_error":null}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'timestamp': '2024-12-03 14:36:33.967817', 'log_type': 'INFO', 'status': 'Successful', 'description': \"'FT_Progression_Analysis_Unseen' notebook completed successfully.\", 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\n{'timestamp': '2024-12-03 14:36:33.967870', 'log_type': 'INFO', 'status': 'Successful', 'description': 'New predictions generated for new students.', 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\n{'timestamp': '2024-12-03 14:36:34.041938', 'log_type': 'INFO', 'status': 'Successful', 'description': 'Bi-weekly run date updated for BL.', 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\n"]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"b084b002-b542-4af4-b979-cbd3ec86c347\",\"activityId\":\"98cabff6-499e-4ae9-919c-8fd87bcf6101\",\"applicationId\":\"application_1733236234345_0001\",\"jobGroupId\":\"5\",\"advices\":{\"warn\":4}}"}},"id":"82f52562-7af5-4ffb-96a2-8ac3a02bad62"},{"cell_type":"code","source":["# Extract the last biweekly run date\n","dl_last_biweekly_run = datetime.strptime(config[\"pipeline_schedule\"][\"dl_last_biweekly_run\"], '%Y-%m-%d')\n","\n","# Get today's date\n","today = datetime.today()\n","\n","# Check if two weeks (14 days) have passed since the last biweekly run\n","if today.date() >= (dl_last_biweekly_run.date() + timedelta(days=14)):\n","    logging(LogLevel.INFO.value,f\"Day for new unseen batch prediction : {today}. Starting Batch Prediction notebook...\",status=\"Successful\")\n","    ####################################\n","    # Execute Notebook: Unseen Prediction\n","    ####################################\n","    execute_notebook_name = \"PT_Progression_Analysis_Unseen\"\n","    try:\n","        logging(LogLevel.INFO.value,f\"Starting '{execute_notebook_name}' notebook...\",status=\"Successful\")\n","\n","        # Run the data_extraction_validation notebook\n","        result = mssparkutils.notebook.run(execute_notebook_name, 86000)\n","        \n","        logging(LogLevel.INFO.value,f\"'{execute_notebook_name}' notebook completed successfully.\",status=\"Successful\")\n","        logging(LogLevel.INFO.value,f\"New predictions generated for new students.\",status=\"Successful\")\n","\n","        # Update the last_biweekly_run date to today and save it back to the JSON file\n","        config[\"pipeline_schedule\"][\"dl_last_biweekly_run\"] = today.strftime('%Y-%m-%d')\n","\n","        with open(\"/lakehouse/default/Files/progression_config_template/progression_config.json\", \"w\") as bi_weekly_config:\n","            json.dump(config, bi_weekly_config)\n","            bi_weekly_config.close()\n","\n","        logging(LogLevel.INFO.value,f\"Bi-weekly run date updated for DL.\",status=\"Successful\")\n","\n","    except Exception as e:\n","        logging(LogLevel.ERROR.value,f\"Error running '{execute_notebook_name}' notebook: {e}\",status=\"Failed\")\n","        createLogfile()\n","        raise e"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"98cabff6-499e-4ae9-919c-8fd87bcf6101","normalized_state":"finished","queued_time":"2024-12-03T14:35:11.1033776Z","session_start_time":null,"execution_start_time":"2024-12-03T14:36:36.5922385Z","execution_finish_time":"2024-12-03T14:37:24.9494437Z","parent_msg_id":"c7f27462-88b4-42e1-a523-902dc82836d6"},"text/plain":"StatementMeta(, 98cabff6-499e-4ae9-919c-8fd87bcf6101, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'timestamp': '2024-12-03 14:36:36.453318', 'log_type': 'INFO', 'status': 'Successful', 'description': 'Day for new unseen batch prediction : 2024-12-03 14:36:36.453121. Starting Batch Prediction notebook...', 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\n{'timestamp': '2024-12-03 14:36:36.453386', 'log_type': 'INFO', 'status': 'Successful', 'description': \"Starting 'PT_Progression_Analysis_Unseen' notebook...\", 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.mssparkutilsrun-result+json":{"run_id":"2f53b983-9566-46b6-93d3-25df26b0abc7","in_pipeline":false,"notebook_name":"PT_Progression_Analysis_Unseen","snapshot_path":"","error":null,"session_id":"98cabff6-499e-4ae9-919c-8fd87bcf6101","spark_pool":"Starter Pool","capacity_id":"ADAB8840-6773-4F35-A058-47ECC22850EA","workspace_id":"21762bb0-73d6-4005-852d-a2d1ab125ffe","root_artifact_id":"b084b002-b542-4af4-b979-cbd3ec86c347","artifact_id":"1a972af0-9b4b-4dd8-8e08-5bab6d7fed68","snapshot_status":"success","snapshot_error":null}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'timestamp': '2024-12-03 14:37:24.389893', 'log_type': 'INFO', 'status': 'Successful', 'description': \"'PT_Progression_Analysis_Unseen' notebook completed successfully.\", 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\n{'timestamp': '2024-12-03 14:37:24.389970', 'log_type': 'INFO', 'status': 'Successful', 'description': 'New predictions generated for new students.', 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\n{'timestamp': '2024-12-03 14:37:24.469830', 'log_type': 'INFO', 'status': 'Successful', 'description': 'Bi-weekly run date updated for DL.', 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\n"]}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"b084b002-b542-4af4-b979-cbd3ec86c347\",\"activityId\":\"98cabff6-499e-4ae9-919c-8fd87bcf6101\",\"applicationId\":\"application_1733236234345_0001\",\"jobGroupId\":\"6\",\"advices\":{\"warn\":4}}"}},"id":"6cafee2e-4df9-4724-bd44-a144cd9cd988"},{"cell_type":"code","source":["logging(LogLevel.INFO.value,f\"Daily Scheduled Trigger Notebook run for Progression complete.\",status=\"Successful\")\n","createLogfile()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"98cabff6-499e-4ae9-919c-8fd87bcf6101","normalized_state":"finished","queued_time":"2024-12-03T14:35:11.4705155Z","session_start_time":null,"execution_start_time":"2024-12-03T14:37:25.4735743Z","execution_finish_time":"2024-12-03T14:37:26.9923654Z","parent_msg_id":"4a22bc6d-abe5-4a97-b4f4-3645998bdb9f"},"text/plain":"StatementMeta(, 98cabff6-499e-4ae9-919c-8fd87bcf6101, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'timestamp': '2024-12-03 14:37:25.376410', 'log_type': 'INFO', 'status': 'Successful', 'description': 'Daily Scheduled Trigger Notebook run for Progression complete.', 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\n{'timestamp': '2024-12-03 14:37:26.604762', 'log_type': 'INFO', 'status': 'Successful', 'description': \"Logging information saved in table 'SilverData.progression_log_table'\", 'notebook': 'Scheduled_Trigger_Notebook_Progression'}\nLogging information saved in table 'SilverData.progression_log_table'.\n"]}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"096eb649-a103-4e0c-84b1-d6344b9673c6"},{"cell_type":"code","source":["# display(spark.sql(\"select * from silverdata.progression_log_table order by timestamp desc\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"ef4fe631-9fb6-4864-aae6-aa2a6aab52a3"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"345cc153-7d3a-431b-a080-e6ffb83063f8","default_lakehouse_name":"SilverData","default_lakehouse_workspace_id":"21762bb0-73d6-4005-852d-a2d1ab125ffe"}}},"nbformat":4,"nbformat_minor":5}